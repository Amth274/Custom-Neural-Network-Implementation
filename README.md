# Custom Neural Network Implementation

## Introduction

This project is a custom implementation of a neural network from scratch, designed for multi-class classification. The neural network supports various optimization algorithms, including Stochastic Gradient Descent (SGD), Momentum, Adagrad, RMSprop, and Adam. The implementation covers forward propagation, backpropagation, and parameter updates, with options for mini-batch and full-batch gradient descent.

## Features

- **Feedforward Neural Network**: Custom implementation from scratch.
- **Activation Functions**: ReLU (Rectified Linear Unit) and Sigmoid.
- **Optimization Algorithms**: SGD, Momentum, Adagrad, RMSprop, and Adam.
- **Training**: Mini-batch and full-batch gradient descent.
- **Loss Function**: Cross-entropy loss for multi-class classification.
- **Model Architecture Visualization**: Displays the architecture of the network.

## Prerequisites

- Python 3.7 or higher
- NumPy library

You can install NumPy using pip:
```bash
pip install numpy
