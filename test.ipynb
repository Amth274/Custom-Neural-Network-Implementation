{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Optimizer:\n",
    "    def __init__(self, optimizer_name, layers, learning_rate=0.01):\n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights_cache = {}\n",
    "        self.bias_cache = {}\n",
    "        self.accumulated_gradients = {}\n",
    "\n",
    "        # Initialize caches for weights and biases\n",
    "        for i in range(len(layers) - 1):  # len(layers) - 1 because we're connecting layer i to i+1\n",
    "            weights_key = f'layer{i}_to_layer{i+1}_weights'\n",
    "            bias_key = f'layer{i+1}_bias'\n",
    "\n",
    "            # Initialize caches\n",
    "            self.weights_cache[weights_key] = np.random.rand(layers[i], layers[i+1]) * 0.001  # Small random weights\n",
    "            self.bias_cache[bias_key] = np.zeros(layers[i+1])\n",
    "            self.accumulated_gradients[weights_key] = np.zeros((layers[i], layers[i+1]))\n",
    "\n",
    "    def adagrad_update(self, gradients):\n",
    "        for key in self.weights_cache.keys():\n",
    "            # Update accumulated gradients \n",
    "            self.accumulated_gradients[key] += gradients[key] ** 2\n",
    "            \n",
    "            # Update weights\n",
    "            adjusted_learning_rate = self.learning_rate / (np.sqrt(self.accumulated_gradients[key]) + 1e-8)  # Small value to prevent division by zero\n",
    "            self.weights_cache[key] -= adjusted_learning_rate * gradients[key]\n",
    "\n",
    "        # Update biases\n",
    "        for bias_key in self.bias_cache.keys():\n",
    "            self.bias_cache[bias_key] -= self.learning_rate * np.sum(gradients[bias_key], axis=0)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, activations, optimizer):\n",
    "        self.layers = layers\n",
    "        self.activations = activations\n",
    "        self.optimizer = optimizer  # Instance of the Optimizer class\n",
    "        self.output_layer_index = len(layers) - 1\n",
    "\n",
    "        self.weights = {}\n",
    "        self.biases = {}\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        for i in range(len(layers) - 1):\n",
    "            weights_key = f'layer{i}_to_layer{i+1}_weights'\n",
    "            bias_key = f'layer{i+1}_bias'\n",
    "            self.weights[weights_key] = np.random.rand(layers[i], layers[i+1]) * 0.01\n",
    "            self.biases[bias_key] = np.zeros(layers[i+1])\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        sig = self.sigmoid(x)\n",
    "        return sig * (1 - sig)\n",
    "\n",
    "    def ReLU(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def ReLU_derivative(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    def activation_function(self, activation_name):\n",
    "        self.activation_dict = {'relu': self.ReLU, 'sigmoid': self.sigmoid}\n",
    "        return self.activation_dict.get(activation_name)\n",
    "\n",
    "    def activation_derivative(self, activation_name):\n",
    "        self.activation_derivative_dict = {'relu': self.ReLU_derivative, 'sigmoid': self.sigmoid_derivative}\n",
    "        return self.activation_derivative_dict.get(activation_name)\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        self.z = {}  # Post activation layer output \n",
    "        self.h = {}  # Pre activation layer output\n",
    "        self.z['layer0_output'] = X \n",
    "\n",
    "        # Loop for forward propagation \n",
    "        for i in range(1, len(self.layers)):\n",
    "            h_key = f'h{i}'\n",
    "            z_key = f'layer{i}_output'\n",
    "            weights_key = f'layer{i-1}_to_layer{i}_weights'\n",
    "\n",
    "            self.h[h_key] = np.dot(self.z[f'layer{i-1}_output'], self.weights[weights_key]) + self.biases[f'layer{i}_bias']\n",
    "            activation_func = self.activation_function(self.activations[i-1])\n",
    "            self.z[z_key] = activation_func(self.h[h_key]) \n",
    "\n",
    "        self.y_hat = self.z[f'layer{self.output_layer_index}_output']\n",
    "\n",
    "    def backward_pass(self, X, y):\n",
    "        self.error = y - self.y_hat\n",
    "        self.errors = {}\n",
    "        self.gradients = {}\n",
    "        activation_derivative = self.activation_derivative(self.activations[self.output_layer_index - 1])\n",
    "        self.errors[f'layer{self.output_layer_index}_error'] = self.error * activation_derivative(self.h[f'h{self.output_layer_index}'])\n",
    "\n",
    "        for i in range(self.output_layer_index, 0, -1):\n",
    "            error_key = f'layer{i}_error'\n",
    "            weights_key = f'layer{i-1}_to_layer{i}_weights'\n",
    "            gradient_key = f'layer{i-1}_to_layer{i}_gradients'\n",
    "\n",
    "            activation_derivative = self.activation_derivative(self.activations[i-1])\n",
    "            self.errors[error_key] = np.dot(self.errors[f'layer{i+1}_error'], self.weights[weights_key].T) * activation_derivative(self.h[f'h{i}'])\n",
    "            self.gradients[gradient_key] = np.dot(self.z[f'layer{i-1}_output'].T, self.errors[error_key])\n",
    "\n",
    "        # Update the weights and biases using the optimizer\n",
    "        self.optimizer.adagrad_update(self.gradients)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = optimizer('adam',[2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_t = NeuralNetwork(layers=[2,3,4,5],optimizer='adam',activations=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer0_to_layer1_weights': 0,\n",
       " 'layer1_to_layer2_weights': 0,\n",
       " 'layer2_to_layer3_weights': 0,\n",
       " 'layer3_to_layer4_weights': 0}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.weights_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [2,3,4,5,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_cache = {\n",
    "    'layer0_to_layer1_weights': np.zeros_like((2,3)),\n",
    "    'layer1_to_layer2_weights': np.random.rand(layer_sizes[1], layer_sizes[2]),\n",
    "    'layer2_to_layer3_weights': np.random.rand(layer_sizes[2], layer_sizes[3]),\n",
    "    'layer3_to_layer4_weights': np.random.rand(layer_sizes[3], layer_sizes[4])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer0_to_layer1_weights': array([0, 0]),\n",
       " 'layer1_to_layer2_weights': array([[0.13439699, 0.36540155, 0.75575309, 0.49598251],\n",
       "        [0.63817102, 0.82166636, 0.0723674 , 0.08654648],\n",
       "        [0.35314526, 0.60408918, 0.93035609, 0.47513007]]),\n",
       " 'layer2_to_layer3_weights': array([[0.37584995, 0.80725366, 0.92150188, 0.41092753, 0.51875249],\n",
       "        [0.06037035, 0.51526596, 0.62878656, 0.56231432, 0.51390748],\n",
       "        [0.52039467, 0.49551973, 0.90495244, 0.71608265, 0.08290224],\n",
       "        [0.7586491 , 0.70018291, 0.64170256, 0.1851104 , 0.85155092]]),\n",
       " 'layer3_to_layer4_weights': array([[0.41696173],\n",
       "        [0.16693957],\n",
       "        [0.36960976],\n",
       "        [0.49062498],\n",
       "        [0.04437588]])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer0_to_layer1_weights': array([[0.9865235 , 0.30182431, 0.78561757],\n",
       "        [0.08596021, 0.7050031 , 0.03825895]]),\n",
       " 'layer1_to_layer2_weights': array([[0.39628808, 0.9524818 , 0.05238355, 0.952711  ],\n",
       "        [0.17458104, 0.75306497, 0.0766882 , 0.10458555],\n",
       "        [0.99134696, 0.56583861, 0.57057921, 0.91813118]]),\n",
       " 'layer2_to_layer3_weights': array([[0.17360458, 0.43922869, 0.93335021, 0.35379363, 0.5940879 ],\n",
       "        [0.80575696, 0.33497127, 0.09345009, 0.95929143, 0.93636888],\n",
       "        [0.62821606, 0.92872066, 0.50534744, 0.4961844 , 0.24726944],\n",
       "        [0.9064185 , 0.10386185, 0.57439881, 0.66205625, 0.08188162]]),\n",
       " 'layer3_to_layer4_weights': array([[0.24282842],\n",
       "        [0.66766086],\n",
       "        [0.58732907],\n",
       "        [0.2450482 ],\n",
       "        [0.22654418]])}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_cache"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "k_",
   "language": "python",
   "name": "k_"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
