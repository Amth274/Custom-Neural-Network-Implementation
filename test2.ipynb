{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import x\n",
    "import numpy as np \n",
    "\n",
    "class Optimizer:\n",
    "    def __init__(self, layers, optimizer_type='sgd', learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-08):\n",
    "        self.optimizer_type = optimizer_type\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon \n",
    "        self.v = {}\n",
    "        self.s = {}\n",
    "        self.layers = layers\n",
    "\n",
    "        if(optimizer_type=='adagrad'):\n",
    "            # self.t = 0\n",
    "            self.weights_cache = {}\n",
    "            self.bias_cache = {}\n",
    "            for i in range(len(self.layers)):\n",
    "                weight_cache_key = f'layer{i}_to_layer{i+1}_weights_cache'\n",
    "                biase_cache_key = f'layer{i}_bias'\n",
    "                self.weights_cache[weight_cache_key] = 0\n",
    "                self.bias_cache[biase_cache_key] = 0\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def update_parameters(self, neural_net, gradients):\n",
    "        if self.optimizer_type == 'sgd':\n",
    "            self.sgd_update(neural_net, gradients)\n",
    "        elif self.optimizer_type == 'momentum':\n",
    "            self.momentum_update(neural_net, gradients)\n",
    "        elif self.optimizer_type == 'adagrad':\n",
    "            self.adagrad_update(neural_net, gradients)\n",
    "        elif self.optimizer_type == 'rmsprop':\n",
    "            self.rmsprop_update(neural_net, gradients)\n",
    "        elif self.optimizer_type == 'adam':\n",
    "            self.adam_update(neural_net, gradients)\n",
    "\n",
    "\n",
    "    def sgd_update(self,neural_net,gradients):\n",
    "        for key in gradients:\n",
    "            neural_net.weights[key] -= self.learning_rate*gradients[key]\n",
    "\n",
    "    \n",
    "    def momentum_update(self,neural_net,  gradients):\n",
    "        for key in gradients:\n",
    "            if key not in self.v:\n",
    "                self.v[key] = np.zeros_like(gradients[key])\n",
    "\n",
    "            self.v[key] = self.bet1*self.v[key]  + (1-self.beta1)*gradients[key]\n",
    "            neural_net.weights -= self.learning_rate*self.v[key]\n",
    "\n",
    "\n",
    "    def adagrad_update(self, neural_net, gradients,layers):\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def rmsprop_update(self, neural_net, gradients):\n",
    "        for key in gradients:\n",
    "            if key not in self.s:\n",
    "                self.s[key] = np.zeros_like(gradients[key])\n",
    "            self.s[key] = self.beta2 * self.s[key] + (1 - self.beta2) * (gradients[key] ** 2)\n",
    "            neural_net.weights[key] -= self.learning_rate * gradients[key] / (np.sqrt(self.s[key]) + self.epsilon)\n",
    "\n",
    "\n",
    "    def adam_update(self, neural_net, gradients):\n",
    "        self.t += 1\n",
    "        for key in gradients:\n",
    "            if key not in self.v:\n",
    "                self.v[key] = np.zeros_like(gradients[key])\n",
    "            if key not in self.s:\n",
    "                self.s[key] = np.zeros_like(gradients[key])\n",
    "\n",
    "            self.v[key] = self.beta1 * self.v[key] + (1 - self.beta1) * gradients[key]\n",
    "            self.s[key] = self.beta2 * self.s[key] + (1 - self.beta2) * (gradients[key] ** 2)\n",
    "\n",
    "            v_corrected = self.v[key] / (1 - self.beta1 ** self.t)\n",
    "            s_corrected = self.s[key] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            neural_net.weights[key] -= self.learning_rate * v_corrected / (np.sqrt(s_corrected) + self.epsilon)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from  optimizer import optimizer\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, optimizer_type, learning_rate, beta1, beta2, epsilon):\n",
    "        self.weights = {}  # dictionary to store weight matrices\n",
    "        self.biases = {}  # dictionary to store layer biases\n",
    "        self.layers = layers\n",
    "        self.optimizer_type = optimizer_type\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2 \n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.weights[f'layer{i}_to_layer{i+1}_weights'] = np.random.rand(layers[i], layers[i+1])\n",
    "        \n",
    "        for j in range(1, len(layers)):\n",
    "            self.biases[f'layer{j}_bias'] = np.zeros(layers[j])  # setting biases to 0 initially\n",
    "\n",
    "        # Initialize optimizer\n",
    "        self.optimizer = Optimizer(layers, optimizer_type, learning_rate, beta1, beta2, epsilon)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    def get_model_params(self):\n",
    "        return self.weights,self.biases # returns weights and biases of the model\n",
    "\n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self,x):\n",
    "        sig = self.sigmoid(x)\n",
    "        return sig*(1-sig)\n",
    "    \n",
    "    def ReLU(self,x):\n",
    "        return np.maximum(0,x)\n",
    "    \n",
    "    def ReLU_derivative(self,x):\n",
    "        return np.where(x>0,1,0)\n",
    "    \n",
    "\n",
    "    def forward_pass(self,X):\n",
    "\n",
    "        self.z_values = {} #pre-activation values\n",
    "        self.layer_output = {} #post activation values \n",
    "        layer_operations = {}\n",
    "\n",
    "        self.layer_output['layer0_output'] = X\n",
    "\n",
    "        #forward propogation loop excluding the output layer calculations (cause of different activation functions)\n",
    "        for i in range(1,len(self.biases),1): \n",
    "            weight_key = f'layer{i-1}_to_layer{i}_weights'\n",
    "            bias_key = f'layer{i}_bias'\n",
    "            z_key = f'z{i}'\n",
    "            h_key = f'layer{i}_output'\n",
    "\n",
    "            self.z_values[z_key] = np.dot(self.layer_output[f'layer{i-1}_output'],self.weights[weight_key]) + self.biases[bias_key] \n",
    "            self.layer_output[h_key] = self.ReLU(self.z_values[z_key])\n",
    "            # print(self.layer_output[h_key])\n",
    "\n",
    "        # calculating the output layer output using the sigmoid activation \n",
    "        self.output_layer_index = len(self.biases) \n",
    "        output_layer_index = len(self.biases) \n",
    "        weight_key = f'layer{output_layer_index-1}_to_layer{output_layer_index}_weights'\n",
    "        bias_key = f'layer{output_layer_index}_bias'\n",
    "        z_key = f'z{output_layer_index}'\n",
    "        h_key = f'layer{output_layer_index}_output'\n",
    "\n",
    "        self.z_values[z_key] = np.dot(self.layer_output[f'layer{output_layer_index-1}_output'], self.weights[weight_key]) + self.biases[bias_key]\n",
    "        self.layer_output[h_key] = self.sigmoid(self.z_values[z_key])\n",
    "        # print(self.layer_output[f'layer{output_layer_index}_output'])\n",
    "\n",
    "        self.y_hat = self.layer_output[h_key] # final predicted output \n",
    "        \n",
    "\n",
    "\n",
    "    def backward_pass(self,X,y,learning_rate):\n",
    "\n",
    "        # creating dictionary for storing the gradients and errors\n",
    "        self.gradients = {}\n",
    "        self.errors = {}\n",
    "\n",
    "        #calculating the error and gradients for the output layer function seperately (cause it has sigmoid derivative)\n",
    "        self.net_error = y - self.y_hat \n",
    "        self.errors[f'layer{self.output_layer_index}_error'] = self.net_error*self.sigmoid_derivative(self.z_values[f'z{self.output_layer_index}'])\n",
    "        self.gradients[f'layer{self.output_layer_index-1}_to_layer{self.output_layer_index}_weights_gradient'] = np.dot(self.layer_output[f'layer{self.output_layer_index-1}_output'].T,self.errors[f'layer{self.output_layer_index}_error'])*learning_rate\n",
    "\n",
    "\n",
    "        for i in range(self.output_layer_index-1,0,-1):\n",
    "            error_key = f'layer{i}_error'\n",
    "            self.errors[error_key] = np.dot(self.errors[f'layer{i+1}_error'],self.weights[f'layer{i}_to_layer{i+1}_weights'].T)*self.ReLU_derivative(self.z_values[f'z{i}'])\n",
    "\n",
    "        for j in range(self.output_layer_index-1,0,-1):\n",
    "            gradient_key = f'layer{j-1}_to_layer{j}_weights_gradient'\n",
    "            h_key = f'layer{j-1}_output' \n",
    "            error_key = f'layer{j}_error'\n",
    "            self.gradients[gradient_key] = np.dot(self.layer_output[h_key].T,self.errors[error_key])\n",
    "    \n",
    "\n",
    "\n",
    "    # def adagrad(self,epsilon):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def parameter_updates(self,learning_rate):\n",
    "\n",
    "        #loop for parameter updates \n",
    "        for i in range(1,len(self.biases)):\n",
    "            weight_key = f'layer{i-1}_to_layer{i}_weights'\n",
    "            gradient_key = f'layer{i-1}_to_layer{i}_weights_gradient'\n",
    "            error_key = f'layer{i}_error' \n",
    "            bias_key = f'layer{i}_bias'\n",
    "\n",
    "\n",
    "\n",
    "            self.weights[weight_key] = self.weights[weight_key] - self.gradients[gradient_key]\n",
    "            self.biases[bias_key] = self.biases[bias_key] - np.sum(self.errors[error_key],axis=0)*learning_rate\n",
    "\n",
    "    \n",
    "    def train(self, X, y, epochs, batch_size=None,learning_rate=0.01):\n",
    "        for epoch in range(epochs):\n",
    "            if batch_size:\n",
    "                # Mini-batch gradient descent\n",
    "                indices = np.random.permutation(X.shape[0])\n",
    "                X_shuffled = X[indices]\n",
    "                y_shuffled = y[indices]\n",
    "\n",
    "                for i in range(0, X.shape[0], batch_size):\n",
    "                    X_batch = X_shuffled[i:i + batch_size]\n",
    "                    y_batch = y_shuffled[i:i + batch_size]\n",
    "                    y_batch_one_hot = np.eye(np.max(y) + 1)[y_batch]\n",
    "\n",
    "                    self.forward_pass(X_batch)\n",
    "                    self.backward_pass(X_batch, y_batch_one_hot,learning_rate=0.01)\n",
    "                    self.optimizer.update_parameters(self, self.gradients)  # Using optimizer to update parameters\n",
    "            else:\n",
    "                # Full-batch gradient descent\n",
    "                y_one_hot = np.eye(np.max(y) + 1)[y]\n",
    "                self.forward_pass(X)\n",
    "                self.backward_pass(X, y_one_hot)\n",
    "                self.optimizer.update_parameters(self, self.gradients)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                loss = self.compute_loss(y_one_hot)\n",
    "                print(f'Epoch: {epoch}, Loss: {loss}')\n",
    "\n",
    "\n",
    "\n",
    "    def compute_loss(self, y):\n",
    "    # Compute cross-entropy loss for multi-class classification\n",
    "        m = y.shape[0]  # Number of samples\n",
    "        y_true_labels = np.argmax(y, axis=1)  # Get the true labels from one-hot encoding\n",
    "        log_likelihood = -np.log(self.y_hat[np.arange(m), y_true_labels])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss\n",
    "\n",
    "            \n",
    "    def model_architecture(self):\n",
    "        num_layers = len(self.layers)  # Total number of layers, including input, hidden, and output\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                # Input layer\n",
    "                print(f'Layer {i}: Input layer with {self.layers[i]} neurons')\n",
    "\n",
    "            elif i == num_layers - 1:\n",
    "                # Output layer\n",
    "                print(f'Layer {i}: Output layer with {self.layers[i]} neurons (Sigmoid activation)')\n",
    "            else:\n",
    "                # Hidden layers\n",
    "                print(f'Layer {i}: Hidden layer with {self.layers[i]} neurons (ReLU activation)')\n",
    "        \n",
    "            # Print the size of weights between this layer and the next\n",
    "            if i < num_layers - 1:\n",
    "                print(f'   Weights shape: {self.weights[f\"layer{i}_to_layer{i+1}_weights\"].shape}')\n",
    "                print(f'   Biases shape: {self.biases[f\"layer{i+1}_bias\"].shape}')\n",
    "        \n",
    "        print(\"End of Architecture\")\n",
    "\n",
    "\n",
    "        \n",
    "            \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(layers=[2,3,1],optimizer_type='adagrad',learning_rate=0.01,beta1=0.9,beta2=0.999,epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.forward_pass(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(n_samples=1000, n_features=2, n_classes=2):\n",
    "    X = np.random.rand(n_samples, n_features)\n",
    "    y = np.random.randint(0, n_classes, size=n_samples)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_dataset(n_samples=1000, n_features=2, n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1000,1000) and (1,3) not aligned: 1000 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[1;32mIn[29], line 94\u001b[0m, in \u001b[0;36mNeuralNetwork.backward_pass\u001b[1;34m(self, X, y, learning_rate)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer_index\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     93\u001b[0m     error_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_error\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors[error_key] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlayer\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_error\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlayer\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_to_layer\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_weights\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mReLU_derivative(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_values[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer_index\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     97\u001b[0m     gradient_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_to_layer\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_weights_gradient\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1000,1000) and (1,3) not aligned: 1000 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "nn.backward_pass(X,y,0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.64243563, 0.78468864],\n",
       "       [0.53175139, 0.19427961],\n",
       "       [0.36872771, 0.27131184],\n",
       "       ...,\n",
       "       [0.06163709, 0.99141494],\n",
       "       [0.69935879, 0.03260633],\n",
       "       [0.37328838, 0.65743872]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "k_",
   "language": "python",
   "name": "k_"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
